{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Evaluations\n",
    "\n",
    "#### Motivation\n",
    "LeapfrogAI uses RAG to provide context-aware responses to users who have specific data they need to reference. In order to make sure RAG is operating at the levels we need it to, we need to get measurable feedback from our RAG pipeline to make it better. We also need a standard to show to mission heroes that we are in fact operating at that level. We do this with evals. But how do we implement evals?\n",
    "\n",
    "#### Discussion Topics\n",
    "- Quick Intro Context\n",
    "- Framework Demos\n",
    "    - RAGAS\n",
    "    - DeepEval\n",
    "    - Arize Phoenix\n",
    "- Questions/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "What are the pieces of RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag_overview](images/rag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we do evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of LLMs (and more specifically, RAG) is difficult.\n",
    "\n",
    "The tasks given to LLMs often should be judged on requirements that would be very broad, and loosely-defined. For instance, an LLM's answer to a question can be:\n",
    "\n",
    "- not grounded in context\n",
    "- repetitive\n",
    "- grammatically incorrect\n",
    "- Excessively lengthy\n",
    "- incoherent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to go about this:\n",
    "\n",
    "**Human in the Loop:** \\\n",
    "You provide your LLM with a set of questions, and then you evaluate it. This allows you to evaluate your RAG pipeline on a number of tests that can be difficult to automate. This is a very time-consuming process, but it's usually necessary as a baseline.\n",
    "\n",
    "**Heuristics:** \\\n",
    "A heuristic evaluator performs some computation to determine a score. This could be \"reference-free\", meaning the heuristic is checking for basic scenarios like a certain length, non-empty responses, or specific formats (regardless of content). The heuristic could also require a ground-truth, like requiring an output contain specific information. These hard-coded comparisons can be difficult to do given the nature of LLM responses (for example, the correct answer to a given question may be \"3 meters\", but the LLM may respond with \"300 centimeters\", which is still correct, but much harder to catch). \n",
    "\n",
    "**LLM-as-judge:** \\\n",
    "An LLM-as-judge evaluator uses an LLM to score system output. This means you can do evaluations that are both reference-free or require ground-truth in a highly automated way. LLMs can help score evaluations that hueristics can't handle (like evaluating sentiment) and are much faster at doing so than humans. However, incorporating evaluator LLMs adds new layers of complexities and potential points of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most RAG evaluation frameworks revolve around the LLM-as-a-judge approach, with some having the flexibility to handle heuristics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look!\n",
    "\n",
    "Approach:\n",
    "- the use of a relatively simple dataset\n",
    "- the LLM-as-a-judge will be run locally\n",
    "- where possible, reasonings for the scores will be provided\n",
    "- I'll showcase unique aspects of each framework as they apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " </pre>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
